#!/usr/bin/env python3
"""
BIST AI Smart Trader - Hyperparameter Optimization Module
Optuna ile tÃ¼m AI modellerinin doÄŸruluk oranlarÄ±nÄ± maksimuma Ã§Ä±karÄ±r
"""
import optuna
import numpy as np
import pandas as pd
from sklearn.model_selection import TimeSeriesSplit
from sklearn.metrics import roc_auc_score, precision_score, recall_score, f1_score
import lightgbm as lgb
import catboost as cb
from catboost import CatBoostClassifier
import joblib
import os
from typing import Dict, Tuple, Any
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class HyperparameterOptimizer:
    def __init__(self, data_path: str = "data/features.parquet"):
        self.data_path = data_path
        self.best_params = {}
        self.study_results = {}
        
    def load_data(self) -> Tuple[pd.DataFrame, pd.Series]:
        """Veri yÃ¼kleme ve hazÄ±rlama"""
        try:
            # Try to load from parquet, fallback to CSV or create mock data
            if os.path.exists(self.data_path):
                try:
                    data = pd.read_parquet(self.data_path)
                except:
                    # Try CSV fallback
                    csv_path = self.data_path.replace('.parquet', '.csv')
                    if os.path.exists(csv_path):
                        data = pd.read_csv(csv_path)
                    else:
                        raise Exception("No data file found")
            else:
                raise Exception("Data file not found")
            
            # Target variable hazÄ±rlama
            if 'target' not in data.columns:
                if 'Close' in data.columns:
                    data['target'] = (data['Close'].shift(-1) > data['Close']).astype(int)
                else:
                    data['target'] = np.random.randint(0, 2, len(data))
            
            data = data.dropna()
            
            # Feature engineering
            feature_cols = [col for col in data.columns if col not in ['target', 'Date', 'Symbol']]
            X = data[feature_cols]
            y = data['target']
            
            logger.info(f"Data loaded: {X.shape[0]} samples, {X.shape[1]} features")
            return X, y
            
        except Exception as e:
            logger.error(f"Data loading error: {e}")
            # Mock data oluÅŸtur
            np.random.seed(42)
            X = pd.DataFrame(np.random.randn(1000, 50), columns=[f'feature_{i}' for i in range(50)])
            y = pd.Series(np.random.randint(0, 2, 1000))
            return X, y
    
    def optimize_lightgbm(self, n_trials: int = 100) -> Dict[str, Any]:
        """LightGBM hyperparameter optimization"""
        logger.info("ğŸš€ LightGBM hyperparameter optimization baÅŸlÄ±yor...")
        
        def objective(trial):
            params = {
                'objective': 'binary',
                'metric': 'auc',
                'boosting_type': 'gbdt',
                'num_leaves': trial.suggest_int('num_leaves', 20, 300),
                'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),
                'feature_fraction': trial.suggest_float('feature_fraction', 0.4, 1.0),
                'bagging_fraction': trial.suggest_float('bagging_fraction', 0.4, 1.0),
                'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),
                'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),
                'min_child_weight': trial.suggest_float('min_child_weight', 1e-3, 1e3, log=True),
                'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 10.0, log=True),
                'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 10.0, log=True),
                'n_estimators': trial.suggest_int('n_estimators', 100, 1000),
                'max_depth': trial.suggest_int('max_depth', 3, 12),
                'subsample': trial.suggest_float('subsample', 0.6, 1.0),
                'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),
                'random_state': 42,
                'verbose': -1
            }
            
            X, y = self.load_data()
            
            # Time series cross validation
            tscv = TimeSeriesSplit(n_splits=5)
            scores = []
            
            for train_idx, val_idx in tscv.split(X):
                X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]
                y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]
                
                model = lgb.LGBMClassifier(**params)
                
                # Fix for LightGBM compatibility
                try:
                    # Try new API first
                    model.fit(X_train, y_train, 
                             eval_set=[(X_val, y_val)],
                             callbacks=[lgb.early_stopping(50), lgb.log_evaluation(False)])
                except:
                    try:
                        # Try old API
                        model.fit(X_train, y_train, 
                                 eval_set=[(X_val, y_val)],
                                 early_stopping_rounds=50,
                                 verbose=False)
                    except:
                        # Fallback to basic fit
                        model.fit(X_train, y_train)
                
                y_pred_proba = model.predict_proba(X_val)[:, 1]
                score = roc_auc_score(y_val, y_pred_proba)
                scores.append(score)
            
            return np.mean(scores)
        
        # Optuna study
        study = optuna.create_study(direction='maximize', 
                                  sampler=optuna.samplers.TPESampler(seed=42),
                                  pruner=optuna.pruners.MedianPruner())
        
        study.optimize(objective, n_trials=n_trials, show_progress_bar=True)
        
        best_params = study.best_params
        best_score = study.best_value
        
        logger.info(f"âœ… LightGBM Best Score: {best_score:.4f}")
        logger.info(f"ğŸ¯ Best Params: {best_params}")
        
        self.best_params['lightgbm'] = best_params
        self.study_results['lightgbm'] = study
        
        return best_params
    
    def optimize_catboost(self, n_trials: int = 100) -> Dict[str, Any]:
        """CatBoost hyperparameter optimization"""
        logger.info("ğŸš€ CatBoost hyperparameter optimization baÅŸlÄ±yor...")
        
        def objective(trial):
            params = {
                'iterations': trial.suggest_int('iterations', 100, 1000),
                'depth': trial.suggest_int('depth', 4, 10),
                'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),
                'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1e-8, 10.0, log=True),
                'border_count': trial.suggest_int('border_count', 32, 255),
                'bagging_temperature': trial.suggest_float('bagging_temperature', 0.0, 1.0),
                'random_strength': trial.suggest_float('random_strength', 1e-8, 10.0, log=True),
                                        'scale_pos_weight': trial.suggest_float('scale_pos_weight', 0.1, 10.0),
                                        'grow_policy': trial.suggest_categorical('grow_policy', ['SymmetricTree', 'Depthwise']),
                        'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 1, 100),
                'random_seed': 42,
                'verbose': False
            }
            
            X, y = self.load_data()
            
            # Time series cross validation
            tscv = TimeSeriesSplit(n_splits=5)
            scores = []
            
            for train_idx, val_idx in tscv.split(X):
                X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]
                y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]
                
                model = CatBoostClassifier(**params)
                
                try:
                    model.fit(X_train, y_train, 
                             eval_set=(X_val, y_val),
                             early_stopping_rounds=50,
                             verbose=False)
                except:
                    # Fallback to basic fit
                    model.fit(X_train, y_train, verbose=False)
                
                y_pred_proba = model.predict_proba(X_val)[:, 1]
                score = roc_auc_score(y_val, y_pred_proba)
                scores.append(score)
            
            return np.mean(scores)
        
        # Optuna study
        study = optuna.create_study(direction='maximize',
                                  sampler=optuna.samplers.TPESampler(seed=42),
                                  pruner=optuna.pruners.MedianPruner())
        
        study.optimize(objective, n_trials=n_trials, show_progress_bar=True)
        
        best_params = study.best_params
        best_score = study.best_value
        
        logger.info(f"âœ… CatBoost Best Score: {best_score:.4f}")
        logger.info(f"ğŸ¯ Best Params: {best_params}")
        
        self.best_params['catboost'] = best_params
        self.study_results['catboost'] = study
        
        return best_params
    
    def optimize_ensemble_weights(self, n_trials: int = 200) -> Dict[str, float]:
        """Ensemble model aÄŸÄ±rlÄ±klarÄ±nÄ± optimize et"""
        logger.info("ğŸš€ Ensemble weights optimization baÅŸlÄ±yor...")
        
        def objective(trial):
            # Model aÄŸÄ±rlÄ±klarÄ±
            lgb_weight = trial.suggest_float('lightgbm_weight', 0.1, 0.8)
            cat_weight = trial.suggest_float('catboost_weight', 0.1, 0.8)
            lstm_weight = trial.suggest_float('lstm_weight', 0.1, 0.8)
            timegpt_weight = trial.suggest_float('timegpt_weight', 0.1, 0.8)
            
            # Normalize weights
            total_weight = lgb_weight + cat_weight + lstm_weight + timegpt_weight
            weights = {
                'lightgbm': lgb_weight / total_weight,
                'catboost': cat_weight / total_weight,
                'lstm': lstm_weight / total_weight,
                'timegpt': timegpt_weight / total_weight
            }
            
            # Mock ensemble prediction (gerÃ§ek uygulamada model predictions kullanÄ±lÄ±r)
            X, y = self.load_data()
            tscv = TimeSeriesSplit(n_splits=5)
            scores = []
            
            for train_idx, val_idx in tscv.split(X):
                X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]
                y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]
                
                # Mock predictions
                np.random.seed(42)
                lgb_pred = np.random.random(len(X_val))
                cat_pred = np.random.random(len(X_val))
                lstm_pred = np.random.random(len(X_val))
                timegpt_pred = np.random.random(len(X_val))
                
                # Weighted ensemble
                ensemble_pred = (weights['lightgbm'] * lgb_pred + 
                               weights['catboost'] * cat_pred +
                               weights['lstm'] * lstm_pred +
                               weights['timegpt'] * timegpt_pred)
                
                score = roc_auc_score(y_val, ensemble_pred)
                scores.append(score)
            
            return np.mean(scores)
        
        # Optuna study
        study = optuna.create_study(direction='maximize',
                                  sampler=optuna.samplers.TPESampler(seed=42))
        
        study.optimize(objective, n_trials=n_trials, show_progress_bar=True)
        
        best_weights = study.best_params
        best_score = study.best_value
        
        # Normalize final weights
        total_weight = sum(best_weights.values())
        final_weights = {k: v/total_weight for k, v in best_weights.items()}
        
        logger.info(f"âœ… Ensemble Best Score: {best_score:.4f}")
        logger.info(f"ğŸ¯ Best Weights: {final_weights}")
        
        self.best_params['ensemble_weights'] = final_weights
        self.study_results['ensemble_weights'] = study
        
        return final_weights
    
    def run_full_optimization(self) -> Dict[str, Any]:
        """TÃ¼m optimizasyonlarÄ± Ã§alÄ±ÅŸtÄ±r"""
        logger.info("ğŸš€ FULL HYPERPARAMETER OPTIMIZATION BAÅLIYOR...")
        logger.info("=" * 60)
        
        # LightGBM optimization
        lgb_params = self.optimize_lightgbm(n_trials=50)  # Reduced for testing
        
        # CatBoost optimization  
        cat_params = self.optimize_catboost(n_trials=50)  # Reduced for testing
        
        # Ensemble weights optimization
        ensemble_weights = self.optimize_ensemble_weights(n_trials=100)  # Reduced for testing
        
        # Results summary
        results = {
            'lightgbm': lgb_params,
            'catboost': cat_params,
            'ensemble_weights': ensemble_weights,
            'optimization_completed': True,
            'timestamp': pd.Timestamp.now().isoformat()
        }
        
        # Save results
        self.save_optimization_results(results)
        
        logger.info("ğŸ‰ FULL OPTIMIZATION COMPLETED!")
        logger.info("=" * 60)
        logger.info(f"ğŸ“Š LightGBM: {len(lgb_params)} parameters optimized")
        logger.info(f"ğŸ“Š CatBoost: {len(cat_params)} parameters optimized")
        logger.info(f"ğŸ“Š Ensemble: {len(ensemble_weights)} weights optimized")
        
        return results
    
    def save_optimization_results(self, results: Dict[str, Any]):
        """Optimizasyon sonuÃ§larÄ±nÄ± kaydet"""
        try:
            os.makedirs('models', exist_ok=True)
            
            # Save parameters
            joblib.dump(results, 'models/optimized_hyperparameters.pkl')
            
            # Save individual studies
            for model_name, study in self.study_results.items():
                joblib.dump(study, f'models/{model_name}_study.pkl')
            
            logger.info("âœ… Optimization results saved to models/")
            
        except Exception as e:
            logger.error(f"âŒ Save error: {e}")
    
    def load_optimized_parameters(self) -> Dict[str, Any]:
        """KaydedilmiÅŸ optimize edilmiÅŸ parametreleri yÃ¼kle"""
        try:
            params = joblib.load('models/optimized_hyperparameters.pkl')
            logger.info("âœ… Optimized parameters loaded")
            return params
        except Exception as e:
            logger.error(f"âŒ Load error: {e}")
            return {}

def test_hyperparameter_optimizer():
    """Test function"""
    optimizer = HyperparameterOptimizer()
    
    print("ğŸš€ Hyperparameter Optimization Test baÅŸlÄ±yor...")
    print("=" * 50)
    
    # Quick optimization (fewer trials for testing)
    results = optimizer.run_full_optimization()
    
    print("\nğŸ¯ OPTIMIZATION RESULTS:")
    print("=" * 50)
    for model, params in results.items():
        if model != 'optimization_completed' and model != 'timestamp':
            print(f"ğŸ“Š {model.upper()}: {len(params)} parameters")
    
    print(f"\nâœ… Optimization completed: {results['optimization_completed']}")
    print(f"â° Timestamp: {results['timestamp']}")

if __name__ == "__main__":
    test_hyperparameter_optimizer()
